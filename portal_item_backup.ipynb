{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup Portal Item JSON files\n",
    "Justin Johnson, justinpjohnson@utah.gov\n",
    "\n",
    "- Developed: 30 October, 2024\n",
    "- Updated: June/July, 2025\n",
    "\n",
    "## What this does\n",
    "\n",
    "An issue was reported by users of the Projects portal at UDOT https://projects.udot.utah.gov/portal\n",
    "\n",
    "Items in the Portal were losing changes and edits\n",
    "- Users were seeing edits that they made to web maps, forms, popups, and other configurations revert or entirely disappear after saving.\n",
    "\n",
    "This script\n",
    "- copies the Item json configuration files from the arcgisportal\\content\\items folder to use as either a restore or to compare changes happening to the files from day to day\n",
    "- each daily backup folder is zipped\n",
    "- a retention limit is set to maintain the most recent number of daily backups\n",
    "\n",
    "### Update: 27 June, 2025\n",
    "\n",
    "Added code to:\n",
    "- zip each daily backup folder\n",
    "- specify the number of daily backups to retain\n",
    "\n",
    "### Update: 7 July, 2025\n",
    "\n",
    "- added logging functionality to store a log of each scheduled run\n",
    "- added error checks for file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the archive retention limit\n",
    "# this is the maximum number of zip files that will be retained\n",
    "# Note: this will always be the number of newest files, not necessarily the number of days that are retained\n",
    "\n",
    "retention_limit = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the logging level\n",
    "# this is the minimum logging level that will be retained in all log files and output streams\n",
    "\n",
    "# INFO\n",
    "log_level = logging.INFO\n",
    "\n",
    "# DEBUG\n",
    "# log_level = logging.DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portal folder location storing JSON configuration file for each Item\n",
    "# these are the files getting backed up by this script\n",
    "\n",
    "items_path = Path(r'D:\\arcgisportal\\content\\items')\n",
    "\n",
    "if not items_path.is_dir():\n",
    "    print(\"Path to Items folder is not valid\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create names for timestamped folder and zip archive\n",
    "\n",
    "# Archive folder location\n",
    "# temporarly folder containing copies of the JSON files are stored here\n",
    "# this folder is zipped, then deleted\n",
    "\n",
    "arch_path = Path(r\"D:\\Backups_portal_items\")\n",
    "\n",
    "if not arch_path.is_dir():\n",
    "    print(\"Path to Archive storage folder is not valid\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "arch_name = Path(\"items_\" + datetime.strftime(datetime.today(), r'%Y_%m_%d_%H%M'))\n",
    "# ex: items_2025_06_27_1447\n",
    "\n",
    "# new folder storing the copied JSON files (unzipped)\n",
    "arch_folder = Path(arch_path, arch_name)\n",
    "\n",
    "# filename of archived folder (zipped)\n",
    "arch_zip = Path(arch_path, arch_name).with_suffix(\".zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log File setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to log file\n",
    "logfile = Path(arch_path, \"portal_backup.log\")\n",
    "\n",
    "if not logfile.exists():\n",
    "    try:\n",
    "        logfile.touch()  # not really necessary. Logger will create this if it doesn't exist\n",
    "    except FileNotFoundError:\n",
    "        print(\"Unable to create new log file. Path may be invalid.\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Handler setup\n",
    "\n",
    "There are two Handler objects created below. One will send log messages to the Log file. The other will display log messages to `stderr`\n",
    "- each handler has its own logging level defined, but the `log_level` set above is the absolute minimum for all handlers\n",
    "  - i.e. to send `DEBUG` logs to the log file or `stderr`, the `log_level` must be changed to `DEBUG` above, and in the handlers below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Logger object\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(log_level)\n",
    "\n",
    "# Set the format of the Log messages\n",
    "# ex:  Tue 2025-07-08 10:24:07 - INFO     creating ZIP archive: C:\\test\\Backups_portal_items\\items_2025_07_08_1014.zip\n",
    "\n",
    "# Formatter\n",
    "fmt_str = \"{asctime} - {levelname:<8} {message}\"\n",
    "fmt_date = \"%a %Y-%m-%d %H:%M:%S\"\n",
    "fmt_style = \"{\"\n",
    "\n",
    "log_formatter = logging.Formatter(fmt=fmt_str, datefmt=fmt_date, style=fmt_style)\n",
    "\n",
    "\n",
    "# configure the Logger with a File Handler and output Stream Handler\n",
    "\n",
    "# File Handler (sends log message to log file)\n",
    "log_handler_file = logging.FileHandler(logfile, mode='a', encoding='UTF-8')\n",
    "log_handler_file.setFormatter(log_formatter)\n",
    "\n",
    "\n",
    "# Stream Handler (sends log messages to sys.stderr)\n",
    "log_handler_stream = logging.StreamHandler()\n",
    "log_handler_stream.setLevel(logging.DEBUG)\n",
    "log_handler_stream.setFormatter(log_formatter)\n",
    "\n",
    "\n",
    "# add the Handlers to the Logger object\n",
    "logger.addHandler(log_handler_file)\n",
    "logger.addHandler(log_handler_stream)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the archive folder for the daily backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue 2025-07-08 10:56:41 - INFO     backup start\n",
      "Tue 2025-07-08 10:56:41 - INFO     archive folder created: C:\\test\\Backups_portal_items\\items_2025_07_08_1056\n"
     ]
    }
   ],
   "source": [
    "# check if the current output directory already exists in the backup folder location (it should not).\n",
    "# If not, create it\n",
    "\n",
    "if not arch_folder.is_dir():\n",
    "    try:\n",
    "        Path.mkdir(arch_folder)\n",
    "\n",
    "        # log the start time of the backup\n",
    "        logger.info(\"backup start\")\n",
    "\n",
    "        logger.info(\"archive folder created: \" + str(arch_folder))\n",
    "    except:\n",
    "        logger.critical(\"unable to create archive folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through all files in the Portal Items Folder\n",
    "- find the valid JSON configuration files\n",
    "- copy those to the backup folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the current folders in the Portal Items folder\n",
    "# each folder is an Item ID for an Item in the Portal\n",
    "# some folders may be empty, others may have subfolders\n",
    "\n",
    "# copy the JSON files from each subfolder to the backup folder named with today's date and time\n",
    "\n",
    "def copy_json_files(items_path):\n",
    "\n",
    "    for root, dirs, files in os.walk(items_path):\n",
    "\n",
    "        for file in files:  # returns filename string\n",
    "\n",
    "            # json files for Portal Items are named with 32-character UUID strings\n",
    "            # we don't need to copy any other files (.xml, \\esriinfo folder, or thumbnails, etc)\n",
    "\n",
    "            # check if the filename is a valid UUID (string of 32 hexadecimal digits)\n",
    "            logger.debug(\"file: \" + file)\n",
    "\n",
    "            if len(file) == 32:\n",
    "                try:\n",
    "                    # attempt to convert the filename string to an integer\n",
    "                    test_int = int(file, 16)\n",
    "\n",
    "                    # copy the file\n",
    "                    shutil.copy2(os.path.join(root,file), arch_folder)\n",
    "\n",
    "                    logger.info(str(file) + \" copied\")\n",
    "\n",
    "                except ValueError:\n",
    "                    # the file name is not a valid UUID, which means it's not the Item json we want to copy\n",
    "\n",
    "                    # pass\n",
    "                    logger.debug(str(file) + \" not copied\")\n",
    "\n",
    "    logger.info(\"done copying JSON files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue 2025-07-08 10:56:47 - INFO     000c791c87b24ab8a2d8c431ca597c0d copied\n",
      "Tue 2025-07-08 10:56:47 - INFO     000e17f48f534af18d22d42e0526d8a8 copied\n",
      "Tue 2025-07-08 10:56:47 - INFO     00a094c8674c4377b33c9792855a9194 copied\n",
      "Tue 2025-07-08 10:56:47 - INFO     00a4155d8fae4befadbe61bd6af6836c copied\n",
      "Tue 2025-07-08 10:56:47 - INFO     00a7aa417c774217923a510f2b0221f7 copied\n",
      "Tue 2025-07-08 10:56:47 - INFO     done copying JSON files\n"
     ]
    }
   ],
   "source": [
    "# Run the function to copy the files...\n",
    "\n",
    "copy_json_files(items_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At this point:\n",
    "- the current Portal Item JSON files have been copied from `items_path` to a backup folder `arch_folder`\n",
    "\n",
    "Next:\n",
    "- zip and delete the folder of JSON files\n",
    "- delete the oldest zip files to maintain the file retention limit setting\n",
    "\n",
    "Note: Zip archive file size seems to be about 40% of the unzipped folder size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue 2025-07-08 10:57:04 - INFO     creating ZIP archive: C:\\test\\Backups_portal_items\\items_2025_07_08_1056.zip\n",
      "Tue 2025-07-08 10:57:04 - INFO     archive folder deleted\n"
     ]
    }
   ],
   "source": [
    "# create the zip file and add files to it\n",
    "logger.info(\"creating ZIP archive: \" + str(arch_zip))\n",
    "\n",
    "with ZipFile(arch_zip, mode='w', compression=ZIP_DEFLATED, compresslevel=9) as outzip:\n",
    "\n",
    "    logger.debug(\"opened ZIP file\")\n",
    "\n",
    "    for itemfile in Path(arch_folder).iterdir():\n",
    "\n",
    "        # write the Item JSON file to the zip archive\n",
    "        # by default, this creates a series of subfolders inside the ZIP file, in the same structure as the original\n",
    "        # this makes it diffcult to browse ZIP files and restore their content (x subfolders deep, etc)\n",
    "        # strip out the leading path of subfolders by specifying arcname=itemfile.name\n",
    "\n",
    "        outzip.write(itemfile, arcname=itemfile.name)\n",
    "\n",
    "        logger.debug(str(itemfile) + \" added to ZIP archive\")\n",
    "\n",
    "        # delete the original file\n",
    "        itemfile.unlink()\n",
    "        logger.debug(str(itemfile) + \" deleted from archive folder\")\n",
    "\n",
    "    outzip.close()\n",
    "    logger.debug(\"closed ZIP file\")\n",
    "\n",
    "# remove the empty archive folder\n",
    "arch_folder.rmdir()\n",
    "\n",
    "logger.info(\"archive folder deleted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the oldest archive(s) to maintain the retention limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tue 2025-07-08 10:57:07 - INFO     5 existing ZIP files\n",
      "Tue 2025-07-08 10:57:07 - INFO     backup complete\n"
     ]
    }
   ],
   "source": [
    "# get a list of all zip files in the archive folder\n",
    "\n",
    "filelist = list(arch_path.glob('*.zip'))\n",
    "\n",
    "logger.info(str(len(filelist)) + \" existing ZIP files\")\n",
    "\n",
    "# Sort the filenames in ascending order\n",
    "# this is also chronological order, since filenames are based on date and time of archive\n",
    "filelist.sort()\n",
    "\n",
    "if len(filelist) > retention_limit:\n",
    "\n",
    "    # slice off a list of files to drop from the FRONT of the sorted list\n",
    "    # when sorted alphabetically, the oldest files are first\n",
    "\n",
    "    dropfiles = filelist[:(len(filelist) - retention_limit)]\n",
    "\n",
    "    logger.info(str(len(dropfiles)) + \" removed to maintain retention limit of \" + str(retention_limit) + \" ZIP files\")\n",
    "\n",
    "    for file in dropfiles:\n",
    "        Path.unlink(file)\n",
    "        logger.debug(str(file) + \" deleted\")\n",
    "\n",
    "\n",
    "logger.info(\"backup complete\")\n",
    "# logging.shutdown()  # this might not be needed when running as a script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
