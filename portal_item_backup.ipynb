{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup Portal Item JSON files\n",
    "Justin Johnson, justinpjohnson@utah.gov\n",
    "\n",
    "- Developed: 30 October, 2024\n",
    "- Updated: 27 June, 2025\n",
    "\n",
    "## What this does\n",
    "\n",
    "An issue was reported by users of the Projects portal at UDOT https://projects.udot.utah.gov/portal\n",
    "\n",
    "Items in the Portal were losing changes and edits\n",
    "- Users were seeing edits that they made to web maps, forms, popups, and other configurations revert or entirely disappear after saving.\n",
    "\n",
    "This script\n",
    "- copies the Item json configuration files from the arcgisporta\\content\\items folder to use as either a restore or to compare changes happening to the files from day to day\n",
    "\n",
    "### Update: 27 June, 2025\n",
    "\n",
    "Added code to:\n",
    "- zip each daily backup folder\n",
    "- specify the number of daily backups to retain\n",
    "\n",
    "### Update: 7 July, 2025\n",
    "\n",
    "Added logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile, ZIP_DEFLATED\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the archive retention limit\n",
    "# this is the maximum number of zip files that will be retained\n",
    "# Note: this will always be the number of newest files, not necessarily the number of days that are retained\n",
    "\n",
    "retention_limit = 4\n",
    "\n",
    "debug_level = logging.DEBUG\n",
    "# debug_level = logging.INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portal folder location storing JSON configuration file for each Item\n",
    "# these are the files getting backed up by this script\n",
    "\n",
    "# items_path = Path(r'D:\\arcgisportal\\content\\items') # prod\n",
    "\n",
    "items_path = Path(r'C:\\test\\items') # dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create names for timestamped folder and zip archive\n",
    "\n",
    "# Archive folder location\n",
    "# temporarly folder containing copies of the JSON files are stored here\n",
    "# this folder is zipped, then deleted\n",
    "\n",
    "# arch_path = Path(r\"D:\\Backups_portal_items\") # prod\n",
    "\n",
    "arch_path = Path(r\"C:\\test\\Backups_portal_items\") # dev\n",
    "\n",
    "arch_name = Path(\"items_\" + datetime.strftime(datetime.today(), r'%Y_%m_%d_%H%M'))\n",
    "# ex: items_2025_06_27_1447\n",
    "\n",
    "# new folder storing the copied JSON files (unzipped)\n",
    "arch_folder = Path(arch_path, arch_name)\n",
    "\n",
    "# filename of archived folder (zipped)\n",
    "arch_zip = Path(arch_path, arch_name).with_suffix(\".zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Logger object, configure it with a File Handler and output Formatter\n",
    "\n",
    "# Path to log file\n",
    "logfile = Path(arch_path, \"portal_backup.log\")\n",
    "\n",
    "\n",
    "# create Logger object\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(debug_level)\n",
    "\n",
    "\n",
    "# Formatter\n",
    "fmt_str = \"{asctime} - {levelname:<8} {message}\"\n",
    "fmt_date = \"%a %Y-%m-%d %H:%M:%S\"\n",
    "fmt_style = \"{\"\n",
    "\n",
    "log_formatter = logging.Formatter(fmt=fmt_str, datefmt=fmt_date, style=fmt_style)\n",
    "\n",
    "\n",
    "# Handler\n",
    "log_handler = logging.FileHandler(logfile, mode='a', encoding='UTF-8')\n",
    "log_handler.setFormatter(log_formatter)\n",
    "\n",
    "# add the handler to the Logger object\n",
    "logger.addHandler(log_handler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the current output directory already exists in the backup folder location.  If not, create it.\n",
    "\n",
    "if not arch_folder.is_dir():\n",
    "    try:\n",
    "        Path.mkdir(arch_folder)\n",
    "        logger.info(\"archive folder created: \" + str(arch_folder))\n",
    "    except:\n",
    "        logger.critical(\"unable to create archive folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the current folders in the Portal Items folder\n",
    "# each folder is an Item ID for an Item in the Portal\n",
    "# some folders may be empty, others may have subfolders\n",
    "\n",
    "# copy the JSON files from each subfolder to the backup folder named with today's date and time\n",
    "\n",
    "def copy_json_files(items_path):\n",
    "\n",
    "    for root, dirs, files in os.walk(items_path):\n",
    "\n",
    "        for file in files:  # returns filename string\n",
    "\n",
    "            # json files for Portal Items are named with 32-character UUID strings\n",
    "            # we don't need to copy any other files (.xml, \\esriinfo folder, or thumbnails, etc)\n",
    "\n",
    "            # check if the filename is a valid UUID (string of 32 hexadecimal digits)\n",
    "            logger.debug(file)\n",
    "\n",
    "            if len(file) == 32:\n",
    "                try:\n",
    "                    # attempt to convert the filename string to an integer\n",
    "                    test_int = int(file, 16)\n",
    "\n",
    "                    # copy the file\n",
    "                    shutil.copy2(os.path.join(root,file), arch_folder)\n",
    "\n",
    "                    logger.info(str(file) + \" copied\")\n",
    "\n",
    "                except ValueError:\n",
    "                    # the file name is not a valid UUID, which means it's not the Item json we want to copy\n",
    "\n",
    "                    # pass\n",
    "                    logger.debug(str(file) + \" not copied\")\n",
    "\n",
    "    logger.info(\"Done copying files\")\n",
    "    print('  done\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# copy the files...\n",
    "\n",
    "copy_json_files(items_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point:\n",
    "- the current Portal Item JSON files have been copied from `items_path` to a backup folder `arch_folder`\n",
    "\n",
    "Next:\n",
    "- zip and delete the folder of JSON files\n",
    "- delete the oldest zip files to maintain the file retention limit setting\n",
    "\n",
    "Note: Zip archive file size seems to be about 40% of the unzipped folder size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the folder, then delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the zip file and add files to it\n",
    "with ZipFile(arch_zip, mode='w', compression=ZIP_DEFLATED, compresslevel=9) as outzip:\n",
    "\n",
    "    logger.debug(\"Opening ZIP file\")\n",
    "\n",
    "    for itemfile in Path(arch_folder).iterdir():\n",
    "\n",
    "        # write the Item JSON file to the zip archive\n",
    "        outzip.write(itemfile)\n",
    "\n",
    "        logger.debug(str(itemfile) + \" added to ZIP archive\")\n",
    "\n",
    "        # delete the original file\n",
    "        itemfile.unlink()\n",
    "        logger.debug(str(itemfile) + \" deleted\")\n",
    "\n",
    "# remove the empty archive folder\n",
    "arch_folder.rmdir()\n",
    "\n",
    "logger.info(\"archive folder deleted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the oldest archive(s) to maintain the retention limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all zip files in the archive folder\n",
    "\n",
    "filelist = list(arch_path.glob('*.zip'))\n",
    "\n",
    "logger.info(str(len(filelist)) + \" existing ZIP files\")\n",
    "\n",
    "# Sort the filenames in ascending order\n",
    "# this is also chronological order, since filenames are based on date and time of archive\n",
    "filelist.sort()\n",
    "\n",
    "if len(filelist) > retention_limit:\n",
    "\n",
    "    # slice off a list of files to drop from the FRONT of the sorted list\n",
    "    # when sorted alphabetically, the oldest files are first\n",
    "\n",
    "    dropfiles = filelist[:(len(filelist) - retention_limit)]\n",
    "\n",
    "    logger.info(str(len(dropfiles)) + \" removed to mainain limit of \" + str(retention_limit) + \" ZIP files\")\n",
    "\n",
    "    for file in dropfiles:\n",
    "        Path.unlink(file)\n",
    "\n",
    "logger.info(\"done\")\n",
    "logging.shutdown()  # this might not be needed when running as a script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
